% author:   sam tenka
% change:   2022-08-14
% create:   2022-08-06

%==============================================================================
%====  0.  DOCUMENT SETTINGS  ================================================
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.0. About this Exposition  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  0.0.0. page geometry  ---------------------------------
\documentclass[11pt, justified]{tufte-book}
\geometry{
  left           = 0.90in, % left margin
  textwidth      = 4.95in, % main text block
  marginparsep   = 0.15in, % gutter between main text block and margin notes
  marginparwidth = 2.30in, % width of margin notes
                 % 0.20in  % width from margin to edge
}


%---------------------  0.0.1. math packages  ---------------------------------
\newcommand\hmmax{0} % to allow for more fonts 
\newcommand\bmmax{0} % to allow for more fonts
\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{bm}
\usepackage{euler}
\usepackage{tikz-cd}

\usepackage{array}   % for \newcolumntype macro
\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type
\newcolumntype{C}{>{$}c<{$}} % math-mode version of "c" column type
\newcolumntype{R}{>{$}r<{$}} % math-mode version of "r" column type

%---------------------  0.0.2. graphics packages  -----------------------------
\usepackage{graphicx, xcolor}
\usepackage{float, capt-of}

%---------------------  0.0.3. packages for fancy text  -----------------------
\usepackage{enumitem}\setlist{nosep}

\usepackage{wedn, miama, pbsi}
\usepackage[T1]{fontenc}
\usepackage{listings}
\newcommand*{\mycommentstyle}[1]{%
  \begingroup
    \lstset{columns=fullflexible}%
    \bsifamily
    \bre
    \selectfont
    #1%
  \endgroup
}
\usepackage{xstring}
\usepackage{fontawesome5}

%---------------------  0.043. colors  ----------------------------------------
\definecolor{mblu}{rgb}{0.05, 0.35, 0.70} \newcommand{\blu}{\color{mblu}}
\definecolor{mbre}{rgb}{0.30, 0.45, 0.60} \newcommand{\bre}{\color{mbre}}
\definecolor{mbro}{rgb}{0.60, 0.05, 0.05} \newcommand{\bro}{\color{mbro}}
\definecolor{mcya}{rgb}{0.10, 0.45, 0.45} \newcommand{\cya}{\color{mcya}}
\definecolor{mgre}{rgb}{0.55, 0.55, 0.50} \newcommand{\gre}{\color{mgre}}
\definecolor{mgrn}{rgb}{0.15, 0.65, 0.05} \newcommand{\grn}{\color{mgrn}}
\definecolor{mred}{rgb}{0.90, 0.05, 0.05} \newcommand{\red}{\color{mred}}

\definecolor{mdbre}{rgb}{0.15, 0.23, 0.30} \newcommand{\dbre}{\color{mdbre}}
\definecolor{mdbro}{rgb}{0.30, 0.03, 0.03} \newcommand{\dbro}{\color{mdbro}}


%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.1. Headers and References  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  0.1.0. intra-document references  ---------------------
\newcommand{\offour}[1]{
    {\tiny \raisebox{0.04cm}{\scalebox{0.9}{$\substack{
        \IfSubStr{#1}{0}{{\blacksquare}}{\square}   
        \IfSubStr{#1}{1}{{\blacksquare}}{\square} \\ 
        \IfSubStr{#1}{2}{{\blacksquare}}{\square}   
        \IfSubStr{#1}{3}{{\blacksquare}}{\square}   
    }$}}}%
}

\newcommand{\offourline}[1]{
    {\tiny \raisebox{0.04cm}{\scalebox{0.9}{$\substack{
        \IfSubStr{#1}{0}{{\blacksquare}}{\square}   
        \IfSubStr{#1}{1}{{\blacksquare}}{\square}
        \IfSubStr{#1}{2}{{\blacksquare}}{\square}   
        \IfSubStr{#1}{3}{{\blacksquare}}{\square}   
    }$}}}%
}
\newcommand{\notesam}[1]{{\blu \textsf{#1}}}
\newcommand{\attn}[1]{{\bro \textsf{#1}}}
\newcommand{\attnsam}[1]{{\red \textsf{#1}}}

\newcommand{\blarr}{\hspace{-0.15cm}${\bro \leftarrow}\,$}
\newcommand{\bcirc}{${\bro ^\circ}$}

\newcounter{footprintssofar}
\setcounter{footprintssofar}{90}
%\newcommand{\plainfootprint}{{\bro \rotatebox{\value{footprintssofar}}{\faIcon{shoe-prints}}}\setcounter{footprintssofar}{\value{footprintssofar}+30} }
\newcommand{\plainfootprint}{}
\newcommand{\footprint}{\marginnote{\plainfootprint} }

%---------------------  0.1.1. table of contents helpers  ---------------------
\newcommand{\phdot}{\phantom{.}}

%---------------------  0.1.2. section headers  -------------------------------
\newcommand{\samtitle} [1]{
  \par\noindent{\Huge \sf \blu #1}
  \vspace{0.4cm}
}

\newcommand{\samquote} [2]{
    \marginnote[-0.4cm]{\begin{flushright}
    \scriptsize
        \gre {\it #1} \\ --- #2
    \end{flushright}}
}

\newcommand{\samsection} [1]{
  \vspace{0.5cm}
  \par\noindent{\LARGE \sf \blu #1}
  \vspace{0.1cm}\par
}

\newcommand{\samsubsection}[1]{
  \vspace{0.3cm}
  \par\noindent{\Large \sf \bre #1}
  \vspace{0.1cm}\par
}

\newcommand{\samsubsubsection}[1]{
   \vspace{0.1cm}
   \par\noindent{\hspace{-2cm}\normalsize \sc \gre #1} ---
}

%---------------------  0.1.3. clear the bibliography's header  ---------------
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.2. Math Symbols and Blocks  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  0.2.0. general math operators  ------------------------
\newcommand{\scirc}{\mathrel{\mathsmaller{\mathsmaller{\mathsmaller{\circ}}}}}
\newcommand{\cmop}[2]{{(#1\!\to\!#2)}}

\newcommand{\horifrac}[2]{#1/#2}

\newcommand{\wrap}[1]{\left(#1\right)}
\newcommand{\wabs}[1]{\left|#1\right|}
\newcommand{\wasq}[1]{\left[#1\right]}

%---------------------  0.2.1. probability symbols  ---------------------------
\newcommand{\KL}{\text{KL}}
\newcommand{\EN}{\text{H}}
\newcommand{\note}[1]{{\blu \textsf{#1}}}

%---------------------  0.2.2. losses averaged in various ways  ---------------
\newcommand{\Ein}  {\text{trn}_{\sS}}
\newcommand{\Einb} {\text{trn}_{\check\sS}}
\newcommand{\Einc} {\text{trn}_{\sS\sqcup \check\sS}}
\newcommand{\Egap} {\text{gap}_{\sS}}
\newcommand{\Eout} {\text{tst}}

%---------------------  0.2.3. double-struck and caligraphic upper letters  ---
\newcommand{\Aa}{\mathbb{A}}\newcommand{\aA}{\mathcal{A}}
\newcommand{\Bb}{\mathbb{B}}\newcommand{\bB}{\mathcal{B}}
\newcommand{\Cc}{\mathbb{C}}\newcommand{\cC}{\mathcal{C}}
\newcommand{\Dd}{\mathbb{D}}\newcommand{\dD}{\mathcal{D}}
\newcommand{\Ee}{\mathbb{E}}\newcommand{\eE}{\mathcal{E}}
\newcommand{\Ff}{\mathbb{F}}\newcommand{\fF}{\mathcal{F}}
\newcommand{\Gg}{\mathbb{G}}\newcommand{\gG}{\mathcal{G}}
\newcommand{\Hh}{\mathbb{H}}\newcommand{\hH}{\mathcal{H}}
\newcommand{\Ii}{\mathbb{I}}\newcommand{\iI}{\mathcal{I}}
\newcommand{\Jj}{\mathbb{J}}\newcommand{\jJ}{\mathcal{J}}
\newcommand{\Kk}{\mathbb{K}}\newcommand{\kK}{\mathcal{K}}
\newcommand{\Ll}{\mathbb{L}}\newcommand{\lL}{\mathcal{L}}
\newcommand{\Mm}{\mathbb{M}}\newcommand{\mM}{\mathcal{M}}
\newcommand{\Nn}{\mathbb{N}}\newcommand{\nN}{\mathcal{N}}
\newcommand{\Oo}{\mathbb{O}}\newcommand{\oO}{\mathcal{O}}
\newcommand{\Pp}{\mathbb{P}}\newcommand{\pP}{\mathcal{P}}
\newcommand{\Qq}{\mathbb{Q}}\newcommand{\qQ}{\mathcal{Q}}
\newcommand{\Rr}{\mathbb{R}}\newcommand{\rR}{\mathcal{R}}
\newcommand{\Ss}{\mathbb{S}}\newcommand{\sS}{\mathcal{S}}
\newcommand{\Tt}{\mathbb{T}}\newcommand{\tT}{\mathcal{T}}
\newcommand{\Uu}{\mathbb{U}}\newcommand{\uU}{\mathcal{U}}
\newcommand{\Vv}{\mathbb{V}}\newcommand{\vV}{\mathcal{V}}
\newcommand{\Ww}{\mathbb{W}}\newcommand{\wW}{\mathcal{W}}
\newcommand{\Xx}{\mathbb{X}}\newcommand{\xX}{\mathcal{X}}
\newcommand{\Yy}{\mathbb{Y}}\newcommand{\yY}{\mathcal{Y}}
\newcommand{\Zz}{\mathbb{Z}}\newcommand{\zZ}{\mathcal{Z}}

%---------------------  0.2.4. sans serif and frak lower letters  -------------
\newcommand{\sfa}{\mathsf{a}}\newcommand{\fra}{\mathcal{a}}
\newcommand{\sfb}{\mathsf{b}}\newcommand{\frb}{\mathcal{b}}
\newcommand{\sfc}{\mathsf{c}}\newcommand{\frc}{\mathcal{c}}
\newcommand{\sfd}{\mathsf{d}}\newcommand{\frd}{\mathcal{d}}
\newcommand{\sfe}{\mathsf{e}}\newcommand{\fre}{\mathcal{e}}
\newcommand{\sff}{\mathsf{f}}\newcommand{\frf}{\mathcal{f}}
\newcommand{\sfg}{\mathsf{g}}\newcommand{\frg}{\mathcal{g}}
\newcommand{\sfh}{\mathsf{h}}\newcommand{\frh}{\mathcal{h}}
\newcommand{\sfi}{\mathsf{i}}\newcommand{\fri}{\mathcal{i}}
\newcommand{\sfj}{\mathsf{j}}\newcommand{\frj}{\mathcal{j}}
\newcommand{\sfk}{\mathsf{k}}\newcommand{\frk}{\mathcal{k}}
\newcommand{\sfl}{\mathsf{l}}\newcommand{\frl}{\mathcal{l}}
\newcommand{\sfm}{\mathsf{m}}\newcommand{\frm}{\mathcal{m}}
\newcommand{\sfn}{\mathsf{n}}\newcommand{\frn}{\mathcal{n}}
\newcommand{\sfo}{\mathsf{o}}\newcommand{\fro}{\mathcal{o}}
\newcommand{\sfp}{\mathsf{p}}\newcommand{\frp}{\mathcal{p}}
\newcommand{\sfq}{\mathsf{q}}\newcommand{\frq}{\mathcal{q}}
\newcommand{\sfr}{\mathsf{r}}\newcommand{\frr}{\mathcal{r}}
\newcommand{\sfs}{\mathsf{s}}\newcommand{\frs}{\mathcal{s}}
\newcommand{\sft}{\mathsf{t}}\newcommand{\frt}{\mathcal{t}}
\newcommand{\sfu}{\mathsf{u}}\newcommand{\fru}{\mathcal{u}}
\newcommand{\sfv}{\mathsf{v}}\newcommand{\frv}{\mathcal{v}}
\newcommand{\sfw}{\mathsf{w}}\newcommand{\frw}{\mathcal{w}}
\newcommand{\sfx}{\mathsf{x}}\newcommand{\frx}{\mathcal{x}}
\newcommand{\sfy}{\mathsf{y}}\newcommand{\fry}{\mathcal{y}}
\newcommand{\sfz}{\mathsf{z}}\newcommand{\frz}{\mathcal{z}}

%---------------------  0.2.5. math environments  -----------------------------
\newtheorem*{qst}{Question}
\newtheorem*{thm}{Theorem}
\newtheorem*{lem}{Lemma}
% ...
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}

%==============================================================================
%=====  1.  PROLOGUE  =========================================================
%==============================================================================

\begin{document}
  \samtitle{tabular reinforcement learning}%recitation 11}
      
  \samsection{a map of learning tasks}
    In the 

    Today we'll learn how to solve classical reinforcement learning problems. 
    Compared to by-now supervised learning

    motivating real-world examples
    include robotics, poker, and ad placement.

    \samsubsection{kinds of learning}
      %\samsubsubsection{kinds of learning}
        How do we communicate patterns of desired behavior?  We can teach:\footprint
        \begin{description}
          \item[\textbf{by instruction}:  ]  ``to tell whether a mushroom is poisonous, first look at its gills...'' 
          \item[\textbf{by example}:      ]  ``here are six poisonous fungi; here, six safe ones.  see a pattern?''
          \item[\textbf{by reinforcement}:]  ``eat foraged mushrooms for a month; learn from getting sick.''
        \end{description}
        %
        Machine learning is the art of programming computers to learn from such
        sources.  We've so far focused on the most important case: learning
        from examples.  


        %https://www.desmos.com/calculator/c6fahb1azs


        Initial   

        Learning by example is key to
        the other modes of learning.
        Today we'll discuss learning by reinforcement.

        \emph{Online}

        Stateful (not independent)
        Stateless
        Extreme Stateless: only one input

        The basic framework is that we have several situations and each permits
        several actions.  Taking an action in a given situation leads to some
        (potentially noisy) reward.  We would like to learn a how to act in 
        any given situation so as to enjoy a high average reward.
        
        \begin{marginfigure}
          \centering
          \begin{tabular}{c|cc}
                               & \textsc{examples}   & \textsc{rewards} \\\hline
              \textsc{one }    & unsupervised        & bandit           \\      
              \textsc{many}    &   supervised        & cond.\ bandit    \\      
              \textsc{sequence}& imitation           & classical RL             
          \end{tabular}
          \caption{%
            kinds of learning
          }
        \end{marginfigure}

      \samsubsection{bias-variance; exploitation-exploration}
 
        reinforcement --> explore vs exploit

        online

        stateful (fully observable / approximation of infinite state)

        P vs NP

      \samsubsection{tabular vs leveraged; local gradient updates}

        \pagebreak
  \samsection{bandits}

    \samsubsection{unconditional}

      \samsubsubsection{setup}
        Okay, let's say that each action $a\in \aA$ in a known set of available
        actions leads to a reward sampled randomly independently from some
        unknown distribution $p_{\sfr;\aA}$. 
        %
        To keep things simple at the start, let's say $\aA$ is finite and
        $\sfr$ takes values in $\{0,1\}$.  Thus, the world is determined by a
        probability $p_a$ of nonzero reward for each action $a$.  In fact,
        let's say there are only two actions: $a^\star = \text{argmax}_a p_a$
        with reward $p^\star$ and $a^\circ$ with reward $p^\circ =
        p^\star-\Delta$.  And let's say we go for some large number $T$ of
        steps. 

        How high can we get our randomness-averaged time-totaled reward?  It
        seems reasonable to compare this amount to the randomness-averaged
        time-totaled reward we always did the most rewarding action
        $a^\star$.  We want to minimize the difference, i.e., our
        expected ``\textbf{regret}''.

        Intuitively, if $\Delta \lesssim 1/\!\sqrt{T}$ we won't ever really know
        which action is best.  In this case we don't expect to do better than
        $\sqrt{T}$ expected regret.  So below we'll \attn{assume} $1/\sqrt{T}
        \leq \Delta$.
        %
        %%At the other extreme, if $\Delta \sim 1$ remains at some constant like 
        %%$0.1$ even as $T$ grows, then 

        Here's one strategy: we could sample each $a$ say, $K=10$ times,
        thereby estimating each $\hat p_a$.  From then on, we'd stick to $\hat
        a^\star = \text{argmax}_a \hat p_a$.  (Say $|\aA| K \ll T$ so that this
        latter step dominates).  Our expected regret then grows linearly with
        $T$:\marginnote{%
          \attn{Exercise}:
          By the way, is $\hat p_{\hat a^\star}$ the same as $p_{\hat a^\star}$
          on average?
        }
        \begin{align*}
          \Ee \, \text{regret}_T \approx\,
          \rR \,\triangleq\,
             %&+        K \cdot \text{avg}_a \wabs{p_{a^\star} - p_a} && \leftarrow \textbf{explo\emph{ration} term} \\
             &+ \Delta K                                             && \leftarrow \textbf{explo\emph{ration} term} \\
             &+ \Delta T \cdot \Pp \wabs{\hat a^\star \neq a^\star}  && \leftarrow \textbf{explo\emph{itation} term}  
        \end{align*}

        Intuitively, $K$ trades off between the opportunity costs near the
        beginning and in the remainder of our timesteps.  Larger $K$ means we
        pay a slower start in return for more accurate estimates that'll inform
        us for the rest of time.

        Our cost is at most
        $$
          \Delta (K + T \exp(-K\Delta^2)) % forget the inverse-linear and the normalization factor...
        $$

      \samsubsubsection{oracle case}
        Supposing we did know $\Delta$ and $T$, a best $K$ would 
        balance the marginal cost $\partial K / \partial K = 1$ of longer exploration
        against the marginal benefit $-\partial T \exp(-K\Delta^2) / \partial K = -T\Delta^2 \exp(-K\Delta^2)$
        of more accurate exploitation:
        %$
        %  1 = T\Delta^2 \exp(-K\Delta^2) 
        %$
        %or
        %$
        %   1/(T\Delta^2) = \exp(-K\Delta^2)
        %$
        %or
        $$
           K = \log(T\Delta^2)/\Delta^2
        $$
        Then we incur cost\bcirc\marginnote{%
            \blarr Note that IF we over-explore by setting $K =
            \log(T)/\Delta^2$ --- that is, by replacing $T\Delta^2$ by $T$ ---
            THEN our cost undergoes the same substitution to become
            $(\log(T)+1)/\Delta$.
            %
            With $\Delta = \rho/\!\sqrt{T}$, this cost grows with $T$ like
            $\rR \sim \log(T) \sqrt{T}$.
        }
        $$
          %\Delta (\log(T\Delta^2)/\Delta^2 + T/(T\Delta^2))
          %=
          \rR = \frac{\log(T\Delta^2)+1}{\Delta}
        $$
        With $\Delta = \rho/\!\sqrt{T}$ for $1 \leq \rho$, this
        %is
        %$
        %((2\log \rho + 1)/\rho)  \cdot \sqrt{T} 
        %$,
        %which
        grows with $T$ like $\rR \sim \sqrt{T}$.

        \vfill
      \samsubsubsection{realistic case}
        We don't know $T, \Delta$.  
        %
        But, astonishingly, there is an algorithm that has expected regret
        nearly as low as when we \emph{do} know $T, \Delta$.

        Intuitively, we want it to ``explore'' as many times $K$
        as if we knew $T, \Delta$.  But
        now, whether or not we do explore at timestep $t$ may depend
        only on the length-$t$ list of previous actions and rewards rather
        than on $\Delta$ and $T$.

        Here's the clever insight: let's \emph{estimate} $\Delta$ and $T$ as
        follows: $T \approx t$ and $\Delta \approx |\hat p_\star - \hat
        p_\circ|$.  After all, on average $t$ is no more than a factor $1/2$   
        off from the true $T$ --- and since $T$ appears in a log, it is
        plausible that we may neglect this difference.
        %
        \attnsam{TODO: ``after all'' heuristic for $\Delta$}

        %
        So, as long as the number $\hat K$ of `explorations' so far is
        $
            \hat K < \log(T)/\Delta^2
        $,
        we'll want to do more exploration.
        Let's write $\hat K = \min(N_\circ, N_\star)$, the least number of
        experiencies we've had among actions.  Then we want to try the
        empirically worse action $\hat a_\circ$ whenever:
        %
        $
            \hat p_\star < \hat p_\circ + \sqrt{\horifrac{\log(T)}{\min(N_\circ, N_\star)}}  
        $.
        With the understanding that $N_\star$ will typically be much bigger than
        $N_\circ$, the above condition is almost the same as this more symmetrical
        condition:
        $$
            \hat p_\star + \sqrt{\horifrac{\log(t)}{N_\star}} < \hat p_\circ + \sqrt{\horifrac{\log(t)}{N_\circ}}  
        $$

        We now have a nice interpretation: at each point in time we choose
        an action with `highest potential', where we judge an action $a$'s potential
        not only by its empirical reward $\hat p_a$ but also by an uncertainty score
        or \textbf{explorer's bonus} $\sqrt{\log(t)/N_a}$.
        have high  \emph{}

        \attnsam{TODO: Observe that most exploration usually happens near the very beginning}
        of time.

        As in the previous passage,\bcirc\marginnote{%
            \blarr We may argue similarly as in the previous passage EXCEPT
            that we must be more careful because different actions are no longer
            independent!
        } this incurs
        $$
          \rR \sim \frac{\log(T\Delta^2)+1}{\Delta}
        $$ 
        much cost, which is a $\log(T)$ factor within the oracle case! 

      \samsubsubsection{code}
        Translating the realistic case algorithm to code is straightforward.
        We just have to worry about early-time edge cases where we
        would take logs or reciprocals of zero.  A nice
        way to sweep these issues under the rug is to initialize our counts
        to small nonzero values.

        \begin{lstlisting}[language=Python, basicstyle=\footnotesize\ttfamily, commentstyle=\mycommentstyle, keywordstyle=\bfseries]
    actions = set([...]) 

    tot_reward_by_action = {0.1 for a in actions} # We initialize to small, nonzero 
    nb_trials_by_action = {0.1 for a in actions}  #   values to avoid divide by zero. 
    T = sum(nb_trials_by_action.values()) # This relation we preserve for all time.

    def reward_estimate(a): return tot_reward_by_action[a] / nb_trials_by_action[a] 
    def explorers_bonus(a): return np.sqrt(np.log(T) / nb_trials_by_action[a]) 
    def next_action(a): return max(( reward_estimate(a)
                                    +explorers_bonus(a), a) for a in actions)[1] 
    def learn_from(a, r):
        tot_reward_by_action[a] += r
        nb_trials_by_action[a] += 1
        T += 1
        \end{lstlisting}
        \vfill

    \samsubsection{conditional}

        In the conditional case the reward may depend on some observed input
        $\sfs$ sampled i.i.d.\ from some unknown distribution on a known set $\sS$.
        So the reward structure is determined by the symbol $p_{r|s;\aA}$.


        %It turns out that our original thought experiment with $\Delta\approx
        %1/\sqrt{T}$ is close to worst case.\marginnote{%
        %  If Nature adversarially chooses then tells us $\Delta$, the worst 
        %  choice of $\Delta$ obeys
        %  $$
        %    \frac{2/\Delta}{\Delta} = \frac{\log(T\Delta^2)+1/T}{\Delta^2}
        %  $$
        %  That is:
        %  $$
        %  2 = \log(T\Delta^2)+1/T
        %  $$
        %  That is:
        %  $$
        %    \Delta \approx \sqrt{\exp(2)/T}
        %  $$
        %  which gives expected reward $\approx \sqrt{T}$.
        %  And we'll see that even when we don't know $\Delta$ after nature chooses
        %  $\Delta$, the same roughly holds.
        %}
        %In this case we have cost roughly 
        %$$
        %  \sqrt{T \log T}
        %$$
        
        %
        %exploration bonus: optimism.  uncertainty confidence bound

    %\samsubsection{conditional}

    \pagebreak
  \samsection{imitation}

    \samsubsection{on-policy learning: TD}

    \samsubsection{euler error}

  \samsection{classical RL}

    \samsubsection{on-policy learning: TD}

    \samsubsection{off-policy learning: Q}

    \samsubsection{exploration strategies} % and incentives



  \pagebreak
  \samsubsection{bonus slide: data dependency diagrams}
      Let's visualize how weights and data affect each other in
      four kinds of (online)\bcirc\marginnote{%
        \blarr In \textbf{online} learning, we continuously learn as we receive a
        stream of data.  Our goal is to experience small time-averaged loss.
      } learning problem.
      %
      Solid and dashed arrows depict contrasts and commonalities.
      %
      Here,
      $\textsf{LRN}$ stands for our learner (e.g.\ its weights);
      %$\theta$ stands for weights;
      $s$, for the current prompt or state;
      %$a^\star$, for true answers or ideal actions;
      $\hat a$, for an answer guessed or an action taken;
      $\lL$, for an unbiased estimator of loss-as-a-function-of-action given $s$;
      and $\ell=\lL(\hat a)$, for loss incurred.


      % SUPERVISED
      \samsubsubsection{supervised}
        We learn from \emph{examples} $\lL$.\bcirc\marginnote{%
          \blarr In vanilla supervised learning, $\lL$ is induced from a random
          sample $a$ (conditioned on $s$) of a `true' answer or `ideal'
          action together with some known cost function $c$: $\lL = c(\hat
          a;a)$.
          %
          Knowledge of $\lL$ allows \textbf{contrafactual reasoning}: \emph{how
          much loss would I have incurred had I instead set $\hat a = \cdots$?}
        }
        States are sampled independently.
      
        \tikzcdset{diagrams={arrows={shorten >=-0.5ex,shorten <=-0.5ex}}}

        \noindent%
        \begin{tikzcd}%
            \textsf{LRN}_0 \arrow[rrr, dotted] \arrow[rdd,
        dotted] & [-18pt]
            & [-18pt]             & [-18pt] \textsf{LRN}_1 \arrow[rrr,
            dotted] \arrow[rdd, dotted] & [-18pt]
            & [-18pt]             & [-18pt] \textsf{LRN}_1 \arrow[rrr,
            dotted] \arrow[rdd, dotted] & [-18pt]
            & [-18pt]             & [-18pt] \textsf{LRN}_2 \arrow[rdd,
            dotted] & [-18pt]                                  & [-18pt] \\
            &
            & \ell_0      &
            &
            & \ell_1      &
            &
            & \ell_2      &
            &                                            & \ell_3    \\
            & \hat a_0 \arrow[ru, dotted, bend right=18]
            & \lL_0 \arrow[ruu] \arrow[u, dotted, bend left=18] &
            & \hat a_1 \arrow[ru, dotted, bend right=18]
            & \lL_1 \arrow[ruu] \arrow[u, dotted, bend left=18] &
            & \hat a_2 \arrow[ru, dotted, bend right=18]
            & \lL_2 \arrow[ruu] \arrow[u, dotted, bend left=18] &
            & \hat a_3 \arrow[ru, dotted, bend right=18]                                  &
            \lL_3 \arrow[u, dotted, bend left=18] \\ & s_0 \arrow[ru, dotted]
            \arrow[u, dotted] \arrow[rruuu, dotted, bend right=27] &
            &
            & s_1 \arrow[ru, dotted] \arrow[u, dotted] \arrow[rruuu, dotted, bend right=27]
            &                                             &
            & s_2 \arrow[ru, dotted] \arrow[u, dotted] \arrow[rruuu, dotted, bend right=27]
            &                                             &
            & s_3 \arrow[u, dotted] \arrow[ru, dotted] &
        \end{tikzcd}


     % BANDIT
     \samsubsubsection{conditional bandit}
        We learn from \emph{rewards} $-\ell$.\bcirc\marginnote{%
          \blarr By contrast, mere knowledge of $\ell$ does not allow
          contrafactual reasoning.  In order to find out what would have
          happened had we instead set $\hat a = \cdots$, we'd have to try that
          action out.  That is: \textbf{it costs us to explore} alternative
          actions. 
        }
        States are sampled independently.

        \noindent%
        \begin{tikzcd}%
            \textsf{LRN}_0 \arrow[rrr, dotted] \arrow[rdd,
        dotted] & [-18pt]
            & [-18pt]             & [-18pt] \textsf{LRN}_1 \arrow[rrr,
            dotted] \arrow[rdd, dotted] & [-18pt]
            & [-18pt]             & [-18pt] \textsf{LRN}_1 \arrow[rrr,
            dotted] \arrow[rdd, dotted] & [-18pt]
            & [-18pt]             & [-18pt] \textsf{LRN}_2 \arrow[rdd,
            dotted] & [-18pt]                                  & [-18pt] \\
            &
            & \ell_0 \arrow[ru]     &
            &
            & \ell_1 \arrow[ru]     &
            &
            & \ell_2 \arrow[ru]     &
            &                                            & \ell_3    \\
            & \hat a_0 \arrow[ru, dotted, bend right=18] 
            & \lL_0 \arrow[u, dotted, bend left=18] &
            & \hat a_1 \arrow[ru, dotted, bend right=18] 
            & \lL_1 \arrow[u, dotted, bend left=18] &
            & \hat a_2 \arrow[ru, dotted, bend right=18] 
            & \lL_2 \arrow[u, dotted, bend left=18] &
            & \hat a_3 \arrow[ru, dotted, bend right=18]                                  &
            \lL_3 \arrow[u, dotted, bend left=18] \\ & s_0 \arrow[ru, dotted]
            \arrow[u, dotted] \arrow[rruuu, dotted, bend right=27] &
            &
            & s_1 \arrow[ru, dotted] \arrow[u, dotted] \arrow[rruuu, dotted, bend right=27]
            &                                             &
            & s_2 \arrow[ru, dotted] \arrow[u, dotted] \arrow[rruuu, dotted, bend right=27]
            &                                             &
            & s_3 \arrow[u, dotted] \arrow[ru, dotted] &
        \end{tikzcd}

     % IMITATION 
     \samsubsubsection{(special case of) imitation}
        We learn from \emph{examples} $\lL$.
        Our actions affect future states.

        \noindent%
        \begin{tikzcd}%
            \textsf{LRN}_0 \arrow[rrr, dotted] \arrow[rdd,
        dotted] & [-18pt]
            & [-18pt]             & [-18pt] \textsf{LRN}_1 \arrow[rrr,
            dotted] \arrow[rdd, dotted] & [-18pt]
            & [-18pt]             & [-18pt] \textsf{LRN}_1 \arrow[rrr,
            dotted] \arrow[rdd, dotted] & [-18pt]
            & [-18pt]             & [-18pt] \textsf{LRN}_2 \arrow[rdd,
            dotted] & [-18pt]                                  & [-18pt] \\
            &
            & \ell_0     &
            &
            & \ell_1     &
            &
            & \ell_2     &
            &                                            & \ell_3    \\
            & \hat a_0 \arrow[ru, dotted, bend right=18] \arrow[rrrd, bend right=18]
            & \lL_0 \arrow[ruu] \arrow[u, dotted, bend left=18] &
            & \hat a_1 \arrow[ru, dotted, bend right=18] \arrow[rrrd, bend right=18]
            & \lL_1 \arrow[ruu] \arrow[u, dotted, bend left=18] &
            & \hat a_2 \arrow[ru, dotted, bend right=18] \arrow[rrrd, bend right=18]
            & \lL_2 \arrow[ruu] \arrow[u, dotted, bend left=18] &
            & \hat a_3 \arrow[ru, dotted, bend right=18]                                  &
            \lL_3 \arrow[u, dotted, bend left=18] \\ & s_0 \arrow[ru, dotted]
            \arrow[u, dotted] \arrow[rrr] \arrow[rruuu, dotted, bend right=27] &
            &
            & s_1 \arrow[ru, dotted] \arrow[u, dotted] \arrow[rruuu, dotted, bend right=27]
            \arrow[rrr] &                                             &
            & s_2 \arrow[ru, dotted] \arrow[u, dotted] \arrow[rruuu, dotted, bend right=27]
            \arrow[rrr] &                                             &
            & s_3 \arrow[u, dotted] \arrow[ru, dotted] &
        \end{tikzcd}


      % REINFORCEMENT
      \samsubsubsection{classical reinforcement}
        We learn from \emph{rewards} $-\ell$.
        Our actions affect future states.
        
        \noindent%
        \begin{tikzcd}%
            \textsf{LRN}_0 \arrow[rrr, dotted] \arrow[rdd,
        dotted] & [-18pt]
            & [-18pt]             & [-18pt] \textsf{LRN}_1 \arrow[rrr,
            dotted] \arrow[rdd, dotted] & [-18pt]
            & [-18pt]             & [-18pt] \textsf{LRN}_1 \arrow[rrr,
            dotted] \arrow[rdd, dotted] & [-18pt]
            & [-18pt]             & [-18pt] \textsf{LRN}_2 \arrow[rdd,
            dotted] & [-18pt]                                  & [-18pt] \\
            &
            & \ell_0 \arrow[ru]     &
            &
            & \ell_1 \arrow[ru]     &
            &
            & \ell_2 \arrow[ru]     &
            &                                            & \ell_3    \\
            & \hat a_0 \arrow[ru, dotted, bend right=18] \arrow[rrrd, bend right=18]
            & \lL_0 \arrow[u, dotted, bend left=18] &
            & \hat a_1 \arrow[ru, dotted, bend right=18] \arrow[rrrd, bend right=18]
            & \lL_1 \arrow[u, dotted, bend left=18] &
            & \hat a_2 \arrow[ru, dotted, bend right=18] \arrow[rrrd, bend right=18]
            & \lL_2 \arrow[u, dotted, bend left=18] &
            & \hat a_3 \arrow[ru, dotted, bend right=18]                                  &
            \lL_3 \arrow[u, dotted, bend left=18] \\ & s_0 \arrow[ru, dotted]
            \arrow[u, dotted] \arrow[rrr] \arrow[rruuu, dotted, bend right=27] &
            &
            & s_1 \arrow[ru, dotted] \arrow[u, dotted] \arrow[rruuu, dotted, bend right=27]
            \arrow[rrr] &                                             &
            & s_2 \arrow[ru, dotted] \arrow[u, dotted] \arrow[rruuu, dotted, bend right=27]
            \arrow[rrr] &                                             &
            & s_3 \arrow[u, dotted] \arrow[ru, dotted] &
        \end{tikzcd}
      
\vfill
\end{document}


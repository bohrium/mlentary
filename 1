% author:   sam tenka
% change:   2022-07-05
% create:   2022-07-05

%==============================================================================
%====  0.  DOCUMENT SETTINGS  ================================================
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.0. About this Exposition  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  0.0.0. page geometry  ---------------------------------
\documentclass[11pt, justified]{tufte-book}
\geometry{
  left           = 0.90in, % left margin
  textwidth      = 4.95in, % main text block
  marginparsep   = 0.15in, % gutter between main text block and margin notes
  marginparwidth = 2.30in, % width of margin notes
                 % 0.20in  % width from margin to edge
}

%---------------------  0.0.1. math packages  ---------------------------------
\newcommand\hmmax{0} % to allow for more fonts 
\newcommand\bmmax{0} % to allow for more fonts
\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{bm}
\usepackage{euler}

\usepackage{array}   % for \newcolumntype macro
\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type
\newcolumntype{C}{>{$}c<{$}} % math-mode version of "c" column type
\newcolumntype{R}{>{$}r<{$}} % math-mode version of "r" column type

%---------------------  0.0.2. graphics packages  -----------------------------
\usepackage{graphicx, xcolor}
\usepackage{float, capt-of}

%---------------------  0.0.3. packages for fancy text  -----------------------
\usepackage{enumitem}\setlist{nosep}
\usepackage{listings}
\usepackage{xstring}
\usepackage{fontawesome5}

%---------------------  0.043. colors  ----------------------------------------
\definecolor{mblu}{rgb}{0.05, 0.35, 0.70} \newcommand{\blu}{\color{mblu}}
\definecolor{mbre}{rgb}{0.30, 0.45, 0.60} \newcommand{\bre}{\color{mbre}}
\definecolor{mbro}{rgb}{0.60, 0.05, 0.05} \newcommand{\bro}{\color{mbro}}
\definecolor{mcya}{rgb}{0.10, 0.45, 0.45} \newcommand{\cya}{\color{mcya}}
\definecolor{mgre}{rgb}{0.55, 0.55, 0.50} \newcommand{\gre}{\color{mgre}}
\definecolor{mgrn}{rgb}{0.15, 0.65, 0.05} \newcommand{\grn}{\color{mgrn}}
\definecolor{mred}{rgb}{0.90, 0.05, 0.05} \newcommand{\red}{\color{mred}}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.1. Headers and References  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  0.1.0. intra-document references  ---------------------
\newcommand{\offour}[1]{
    {\tiny \raisebox{0.04cm}{\scalebox{0.9}{$\substack{
        \IfSubStr{#1}{0}{{\blacksquare}}{\square}   
        \IfSubStr{#1}{1}{{\blacksquare}}{\square} \\ 
        \IfSubStr{#1}{2}{{\blacksquare}}{\square}   
        \IfSubStr{#1}{3}{{\blacksquare}}{\square}   
    }$}}}%
}

\newcommand{\offourline}[1]{
    {\tiny \raisebox{0.04cm}{\scalebox{0.9}{$\substack{
        \IfSubStr{#1}{0}{{\blacksquare}}{\square}   
        \IfSubStr{#1}{1}{{\blacksquare}}{\square}
        \IfSubStr{#1}{2}{{\blacksquare}}{\square}   
        \IfSubStr{#1}{3}{{\blacksquare}}{\square}   
    }$}}}%
}
\newcommand{\notesam}[1]{{\blu \textsf{#1}}}
\newcommand{\attn}[1]{{\bro \textsf{#1}}}
\newcommand{\attnsam}[1]{{\red \textsf{#1}}}

\newcommand{\blarr}{\hspace{-0.15cm}${\bro \leftarrow}\,$}
\newcommand{\bcirc}{${\bro ^\circ}$}

%---------------------  0.1.1. table of contents helpers  ---------------------
\newcommand{\phdot}{\phantom{.}}

%---------------------  0.1.2. section headers  -------------------------------
\newcommand{\samtitle} [1]{
  \par\noindent{\Huge \sf \blu #1}
  \vspace{0.4cm}
}

\newcommand{\samquote} [2]{
    \marginnote[-0.4cm]{\begin{flushright}
    \scriptsize
        \gre {\it #1} \\ --- #2
    \end{flushright}}
}

\newcommand{\samsection} [1]{
  \vspace{0.5cm}
  \par\noindent{\LARGE \sf \blu #1}
  \vspace{0.1cm}\par
}

\newcommand{\samsubsection}[1]{
  \vspace{0.3cm}
  \par\noindent{\Large \sf \bre #1}
  \vspace{0.1cm}\par
}

\newcommand{\samsubsubsection}[1]{
   \vspace{0.1cm}
   \par\noindent{\hspace{-2cm}\normalsize \sc \gre #1} ---
}

%---------------------  0.1.3. clear the bibliography's header  ---------------
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.2. Math Symbols and Blocks  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  0.2.0. general math operators  ------------------------
\newcommand{\scirc}{\mathrel{\mathsmaller{\mathsmaller{\mathsmaller{\circ}}}}}
\newcommand{\cmop}[2]{{(#1\!\to\!#2)}}

%---------------------  0.2.1. probability symbols  ---------------------------
\newcommand{\KL}{\text{KL}}
\newcommand{\EN}{\text{H}}
\newcommand{\note}[1]{{\blu \textsf{#1}}}

%---------------------  0.2.2. losses averaged in various ways  ---------------
\newcommand{\Ein}  {\text{trn}_{\sS}}
\newcommand{\Einb} {\text{trn}_{\check\sS}}
\newcommand{\Einc} {\text{trn}_{\sS\sqcup \check\sS}}
\newcommand{\Egap} {\text{gap}_{\sS}}
\newcommand{\Eout} {\text{tst}}

%---------------------  0.2.3. double-struck and caligraphic upper letters  ---
\newcommand{\Aa}{\mathbb{A}}\newcommand{\aA}{\mathcal{A}}
\newcommand{\Bb}{\mathbb{B}}\newcommand{\bB}{\mathcal{B}}
\newcommand{\Cc}{\mathbb{C}}\newcommand{\cC}{\mathcal{C}}
\newcommand{\Dd}{\mathbb{D}}\newcommand{\dD}{\mathcal{D}}
\newcommand{\Ee}{\mathbb{E}}\newcommand{\eE}{\mathcal{E}}
\newcommand{\Ff}{\mathbb{F}}\newcommand{\fF}{\mathcal{F}}
\newcommand{\Gg}{\mathbb{G}}\newcommand{\gG}{\mathcal{G}}
\newcommand{\Hh}{\mathbb{H}}\newcommand{\hH}{\mathcal{H}}
\newcommand{\Ii}{\mathbb{I}}\newcommand{\iI}{\mathcal{I}}
\newcommand{\Jj}{\mathbb{J}}\newcommand{\jJ}{\mathcal{J}}
\newcommand{\Kk}{\mathbb{K}}\newcommand{\kK}{\mathcal{K}}
\newcommand{\Ll}{\mathbb{L}}\newcommand{\lL}{\mathcal{L}}
\newcommand{\Mm}{\mathbb{M}}\newcommand{\mM}{\mathcal{M}}
\newcommand{\Nn}{\mathbb{N}}\newcommand{\nN}{\mathcal{N}}
\newcommand{\Oo}{\mathbb{O}}\newcommand{\oO}{\mathcal{O}}
\newcommand{\Pp}{\mathbb{P}}\newcommand{\pP}{\mathcal{P}}
\newcommand{\Qq}{\mathbb{Q}}\newcommand{\qQ}{\mathcal{Q}}
\newcommand{\Rr}{\mathbb{R}}\newcommand{\rR}{\mathcal{R}}
\newcommand{\Ss}{\mathbb{S}}\newcommand{\sS}{\mathcal{S}}
\newcommand{\Tt}{\mathbb{T}}\newcommand{\tT}{\mathcal{T}}
\newcommand{\Uu}{\mathbb{U}}\newcommand{\uU}{\mathcal{U}}
\newcommand{\Vv}{\mathbb{V}}\newcommand{\vV}{\mathcal{V}}
\newcommand{\Ww}{\mathbb{W}}\newcommand{\wW}{\mathcal{W}}
\newcommand{\Xx}{\mathbb{X}}\newcommand{\xX}{\mathcal{X}}
\newcommand{\Yy}{\mathbb{Y}}\newcommand{\yY}{\mathcal{Y}}
\newcommand{\Zz}{\mathbb{Z}}\newcommand{\zZ}{\mathcal{Z}}

%---------------------  0.2.4. sans serif and frak lower letters  -------------
\newcommand{\sfa}{\mathsf{a}}\newcommand{\fra}{\mathcal{a}}
\newcommand{\sfb}{\mathsf{b}}\newcommand{\frb}{\mathcal{b}}
\newcommand{\sfc}{\mathsf{c}}\newcommand{\frc}{\mathcal{c}}
\newcommand{\sfd}{\mathsf{d}}\newcommand{\frd}{\mathcal{d}}
\newcommand{\sfe}{\mathsf{e}}\newcommand{\fre}{\mathcal{e}}
\newcommand{\sff}{\mathsf{f}}\newcommand{\frf}{\mathcal{f}}
\newcommand{\sfg}{\mathsf{g}}\newcommand{\frg}{\mathcal{g}}
\newcommand{\sfh}{\mathsf{h}}\newcommand{\frh}{\mathcal{h}}
\newcommand{\sfi}{\mathsf{i}}\newcommand{\fri}{\mathcal{i}}
\newcommand{\sfj}{\mathsf{j}}\newcommand{\frj}{\mathcal{j}}
\newcommand{\sfk}{\mathsf{k}}\newcommand{\frk}{\mathcal{k}}
\newcommand{\sfl}{\mathsf{l}}\newcommand{\frl}{\mathcal{l}}
\newcommand{\sfm}{\mathsf{m}}\newcommand{\frm}{\mathcal{m}}
\newcommand{\sfn}{\mathsf{n}}\newcommand{\frn}{\mathcal{n}}
\newcommand{\sfo}{\mathsf{o}}\newcommand{\fro}{\mathcal{o}}
\newcommand{\sfp}{\mathsf{p}}\newcommand{\frp}{\mathcal{p}}
\newcommand{\sfq}{\mathsf{q}}\newcommand{\frq}{\mathcal{q}}
\newcommand{\sfr}{\mathsf{r}}\newcommand{\frr}{\mathcal{r}}
\newcommand{\sfs}{\mathsf{s}}\newcommand{\frs}{\mathcal{s}}
\newcommand{\sft}{\mathsf{t}}\newcommand{\frt}{\mathcal{t}}
\newcommand{\sfu}{\mathsf{u}}\newcommand{\fru}{\mathcal{u}}
\newcommand{\sfv}{\mathsf{v}}\newcommand{\frv}{\mathcal{v}}
\newcommand{\sfw}{\mathsf{w}}\newcommand{\frw}{\mathcal{w}}
\newcommand{\sfx}{\mathsf{x}}\newcommand{\frx}{\mathcal{x}}
\newcommand{\sfy}{\mathsf{y}}\newcommand{\fry}{\mathcal{y}}
\newcommand{\sfz}{\mathsf{z}}\newcommand{\frz}{\mathcal{z}}

%---------------------  0.2.5. math environments  -----------------------------
\newtheorem*{qst}{Question}
\newtheorem*{thm}{Theorem}
\newtheorem*{lem}{Lemma}
% ...
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}

%==============================================================================
%=====  1.  DOCUMENT PROPER  ==================================================
%==============================================================================

\begin{document}

  %\newcommand{}{\textsc{Philip,  Mariana, Mawuko}}
  %\newcommand{ }{\textsc{James,   Bereket, Amadeo}}
  %%
  %\newcommand{}   {\textsc{Philip,  James}}
  %\newcommand{}   {\textsc{Mariana, Bereket}}
  %\newcommand{}   {\textsc{Mawuko,  Amadeo}}
  %
  %\newcommand{\exercise}[2]{\par\noindent\attn{Exercise} (#1): \emph{#2}}
  \newcommand{\exercise}[2]{\par\noindent\attn{Exercise}: \emph{#2}}
  %\newcommand{\exercise}[2]{\marginnote{\attn{Exercise} (#1): \emph{#2}}}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  1.0. front matter  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  \samtitle{rec 07:
            ideas in neural architecture}

  \noindent
    Let's discuss neural networks design choices.  By today's end, we'll be
    able to:\marginnote{%
    Our rough schedule is:
    \begin{description}
        \item[\hspace{0.25cm}] \emph{19:30} neural nets: width and depth
        \item[\hspace{0.25cm}] \emph{19:45} `to build a tool, use it'
        \item[\hspace{0.25cm}] \emph{20:00} latent representations 
        \item[\hspace{0.25cm}] \emph{20:15} symmetries
        \item[\hspace{0.25cm}] \emph{20:30} rnns: backprop practice
        \item[\hspace{0.25cm}] \emph{20:45} rnns: training 
        \item[\hspace{0.25cm}] \emph{21:00} 
    \end{description}
    }
    \begin{description}
        \item[\hspace{1cm}] \emph{tailor} a neural network to a task's inherent
                            dependencies
        \item[\hspace{1cm}] \emph{exploit} a task's symmetries to improve
                            generalization.
        \item[\hspace{1cm}] \emph{train} an RNN to do basic sentiment analysis.
        %\item[\hspace{1cm}] \emph{initialize} weights to avoid stalled training 
            % TODO: also discuss bottlenecks / gradient boosts?
    \end{description}
    As always, \attn{please ask questions} at any time, including by
    interrupting me!

  \samsection{C. automatic featurization, continued}
    %\samsubsection{new features from old}
    %\samsubsection{fitting features to data}
    %\samsubsection{dynamics of shallow net learning}

    %

    \samsubsection{basic architecture: width and depth}
      \samsubsubsection{last time: width}\vspace{-0.5cm}
        \begin{figure}[h]
          \centering
              \includegraphics[width=0.500\textwidth]{shallow}
          \caption{%
            \textbf{Shallow neural nets.} Data flows right to left via {\gre
            gray} transforms.  We use the {\blu blue} quantities to predict;
            the {\color{orange} orange}, to train.  Thin vertical
            strips depict vectors; small squares, scalars.  
            %
            We train the net to maximize data likelihood per its softmax
            predictions:\vspace{-0.1cm}
            %
            $$
                \ell = \textstyle\sum_k s_k 
                \quad
                s_k = y_k \log(1/p_k)
            $$
            \vspace{-0.4cm}
            $$
                p_k = \exp(o_k) /\!\textstyle\sum_{\tilde k} \exp(o_{\tilde k})
            $$
            %
            The decision function $o$ is a linear combination of features $h$
            nonlinearly transformed from $x$:\vspace{-0.1cm}
            $$
                o_k = \textstyle\sum_{j} A_{kj} h_j 
            $$
            Each ``\textbf{hidden activation}'' or ``\textbf{learned feature}''
            $h_j$ measures tresspass past a linear boundary determined by a
            vector $B_j$:\vspace{-0.1cm}
            $$
                h_j = \text{relu}(z_j) = \max(0,z_j)
                \quad
                z_j = \textstyle\sum_{i} B_{ji} x_i 
            $$
          }
        \end{figure}%
      \samsubsubsection{today: also depth}\vspace{-0.5cm}
        \begin{figure}[h]
          \centering
              \includegraphics[width=0.700\textwidth]{deep}
          \caption{%
            \textbf{A deeper neural net.} As before, data flows right to left
            via {\gre gray} transforms.
            %
            We train the net to maximize data likelihood per its softmax
            predictions and the decision function $o$ is a linear combination
            of features $h^2$ nonlinearly transformed from $x$.
            %
            However, that nonlinear transform is now less direct: we transform
            $x$ to a first layer $h^1$ of hidden features and then to $h^2$.
            Explicitly:\vspace{-0.1cm}
            $$
                h^2_j = \text{relu}(z^2_j) 
                \quad
                z^2_j = \textstyle\sum_{i} B_{ji} h^1_i 
            $$
            and\vspace{-0.1cm}
            $$
                h^1_j = \text{relu}(z^2_j)
                \quad
                z^1_j = \textstyle\sum_{i} C_{ji} x_i 
            $$
            We may regard $x$ as $h^0$ and $o$ as $z^3$ if we wish.
            We say this net has a \textbf{depth} of
            \textbf{three weight layers} or of \textbf{two hidden layers}. 
          }
        \end{figure}

        The next three questions consider classifying
        raw dimension-$784$ inputs into $10$ classes.  We'll answer in terms of
        the dimensions (one number for the shallow net; two for the deeper net)
        of the hidden layers.
        %
        \exercise{}{How many parameters does the shallow net have, ignoring
                    bias terms?}
        %
        \exercise{}{How many parameters does the deeper net have, ignoring 
                    bias terms?}
        %
        \exercise{}{How about if we allow bias terms for each weight layer?}

        \newpage
      \samsubsubsection{initialization}
        In the next three questions, we initialize all weights to zero.  We're
        doing binary classification without weight regularization.
        %
        \exercise{}{What is the training loss at initialization?}
        %
        \exercise{}{What is the loss gradient at initialization?}
        %
        \exercise{}{What is the testing loss after a thousand SGD
        updates?}

        Due to the pathology\marginnote{%
          The nonconvexity of learning --- as evidenced by symmetries in the
          model --- allows this pathology.  
        }
        uncovered in the above exercises, we like to \emph{randomly} initialize
        our weights.

        In what follows, we focus on the input-most layer defined on an input
        vector $x$ by $h^1_j = \text{relu}\left(\sum_i C_{ji} x_i \right)$.  We
        assume that $D^1 \times D^0$ many matrix elements $C_{ji}$s are chosen
        independently according to some centered distribution with expected
        absolute value $s$.   
        %
        \exercise{}{If each $|x_i| \approx 1$ then each $|h^1_j| \approx$
        what?}
        %
        \exercise{}{If each $|x_i| \approx 1$ and each $|\partial \ell/\partial
        h_j| \approx 1$ then each $|\partial \ell/\partial C_{ji}| \approx$
        what?}
        %
        \exercise{}{If each $|x_i| \approx 1$ and each $|\partial \ell/\partial
        h_j| \approx 1$ then each $|\partial \ell/\partial x_i| \approx$ what?}

        So for calibrated forward propagation, we might initialize each $C_{ji}$
        to have scale roughly $\sqrt{2/D^0}$.
        %
        For calibrated backward propagation, we might initialize each $C_{ji}$
        to have scale roughly $\sqrt{2/D^1}$.
        %
        We often use a compromise, named after \textbf{Glorot, Bengio, Xavier},
        and probably others:
        $$
            |C_{ji}| \approx \sqrt{\frac{4}{D^1+D^0}}
        $$
        For example, we can initialize by sampling $C_{ji}$ from a centered
        normal with variance $4/(D^1+D^0)$.  The tension between the forward 
        and backward scales is least when .  This is a weak reason to favor
        gradual rather than abrupt changes in dimension across a neural network.

      \samsubsubsection{hyperparameters affect generalization}

        \exercise{}{why is the generalization gap usually positive?}
        \exercise{}{why not just gradient descend on test loss?}

        For the two questions below, we assume a fixed, smallish training set
        size and a fixed, moderate number of gradient descent steps.
        %
        \exercise{}{sketch the training and testing accuracies as a function of
                    hidden dimension.}
        %
        \exercise{}{sketch the training and testing accuracies as a function of
                    the learning rate.}

    \samsubsection{wishful thinking}

      \samsubsubsection{to build a tool, use it}
      \samsubsubsection{architecture, depth, hierarchy}
      \samsubsubsection{latent representations}

    \samsubsection{exploiting symmetry through...}

      Let's help our machine not re-invent the wheel.

      \samsubsubsection{...data augmentation}
      \samsubsubsection{...canonicalization}
      \samsubsubsection{...data abstraction}
      \samsubsubsection{...equivariant architecture}

    \samsubsection{example: recurrent neural networks}

      \samsubsubsection{locality and symmetry: 1d cnn}
      \samsubsubsection{latents and dependencies: rnn}
      \samsubsubsection{forward pass}
      \samsubsubsection{backward pass}

    %\samsubsection{a sneak peak at creative losses} 
      %\samsubsubsection{generation via deconvolution}
      %\samsubsubsection{an example: pix2pix}

\end{document}

